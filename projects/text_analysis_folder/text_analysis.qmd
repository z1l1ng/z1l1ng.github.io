---
title: "Text Analysis of Movies & TV Shows of Streaming Services"
author: "Ziling Zhen"
date: "2024-11-20"
sidebar: false
format:
  html:
    code-fold: true
image: logos.png
editor_options: 
  chunk_output_type: console
description: "Mini-project for SDS 264 - Data Science 2 where we use stingr functions and other text analysis methods"
freeze: true
---

This project uses R to create explore the frequency of words in titles and genres of movies and tv shows of different streaming services by creating visuals such as bar plots, wordclouds and tables.

To view the qmd, click [here](https://github.com/z1l1ng/z1l1ng.github.io/blob/main/projects/text_analysis_folder/text_analysis.qmd).

# Introduction
Do you ever wonder specifically what types of movies or tv shows are on the streaming services you pay for? We will be exploring a comprehensive collection of all titles on Amazon Prime, Apple TV, HBO MAX, Hulu, and Netflix using data collected from IMDB and published on Kaggle. Binding together the rows of these datasets, we end up with 120,930 rows and 9 columns. Each row being title, and columns representing the type, being movie or tv show, the year it was released, genre of the title, imdb rating, the streaming service, etc. 

```{r}
#| include: FALSE
library(cowplot)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(viridis)
library(ggthemes)
library(gutenbergr)
library(rvest)
library(widyr)
library(tm)
library(topicmodels)
library(RCurl)
library(ggraph)
library(igraph)
library(gridExtra)
library(kableExtra)
```

```{r, include = FALSE}
hulu <- read_csv("~/z1l1ng.github.io/projects/text_analysis_folder/hulu_data.csv")
netflix <- read_csv("~/z1l1ng.github.io/projects/text_analysis_folder/netflix_data.csv")
appletv <- read_csv("~/z1l1ng.github.io/projects/text_analysis_folder/appletv_data.csv")
amazonprime <- read_csv("~/z1l1ng.github.io/projects/text_analysis_folder/amazonprime_data.csv")
hbo <- read_csv("~/z1l1ng.github.io/projects/text_analysis_folder/hbo_data.csv")

# new column for service so we know where each title is from
hulu <- hulu |>
  mutate(service = "Hulu")

netflix <- netflix |>
  mutate(service = "Netflix")

hbo <- hbo |>
  mutate(service = "HBO")

appletv <- appletv |>
  mutate(service = "AppleTv") 

amazonprime <- amazonprime |>
  mutate(service = "AmazonPrime") 

# bind all rows for big dataset
big_data <- bind_rows(hulu, netflix, hbo, appletv, amazonprime) 
```

```{r}
# function for plotting since we have to do it twice
count_plot <- function(variable){
  #labs for title
  title_label <- rlang::englue("Number of Titles per {{variable}}") 
  
  #labs for x axis
  xaxis <- rlang::englue("{{variable}}")
  
  #the plot
  big_data |>
  group_by({{variable}}) |>
  count({{variable}}) |>
  ggplot(aes(x = {{variable}}, y = n, fill = {{variable}})) +
  geom_bar(stat = "identity") +
  #adds numbers to bar
  geom_text(aes(label = n)) + 
  labs(
    title = title_label,
    x = xaxis,
    y = "Count"
  ) + 
  theme_bw()
}

title_count <- count_plot(service)

type_count <- count_plot(type)

# aligning grids in html using cowplot
plot_grid(title_count, type_count, ncol = 1, align = "v") 
```

We can see that for most titles to least, it ranks Amazon Prime, Netflix, Apple TV, Hulu and HBO. From our 120,930 titles, we have more movies than tv shows. 

# Genres of Movies 

For genres of movies we will be using a str_split() to split up the lists of genres associated with our titles. Some of these genres are also "Action & Adventure" or "War & Politics", meaning we also have to split the ampersand as well as the comma. After splitting our genres we find that we have 26 distinct genres. We'll now look what the most popular genres are. 

```{r}
# new tibble for count of genres by type
genres_count <- big_data |> 
  
  # splitting by commas, and ampersand
  mutate(genres = str_split(genres, ", | & ")) |> 
  
  # "unlisting"
  unnest(genres) |> 
  
  # getting rid of no genres
  drop_na(genres) |> 
  group_by(genres, type, service) |>
  
  # count genres for movies and tv shows for service
  count(genres) |> 
  ungroup()
```

```{r, include=FALSE}
# for number of distinct genres
big_data |> 
  mutate(genres = str_split(genres, ", | & ")) |>
  unnest(genres) |>
  drop_na(genres) |>
  distinct(genres) 
```

```{r}
genres_count |>
  group_by(service) |>
  
  # filter for movie by service
  filter(type == "movie") |> 
  slice_max(n, n = 10) |>
  mutate(genre = fct_reorder(genres, n)) |>
  ggplot(aes(y = genre, x = n, fill = service)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    facet_wrap(~ service, scales = "free") +
  labs(
    title = "Top Genres 10 of Movies by Streaming Services "
  ) + 
  theme_bw()

genres_count |>
  group_by(service) |>
  
  # filter for tv by service
  filter(type == "tv") |> 
  slice_max(n, n = 10) |>
  mutate(genre = fct_reorder(genres, n)) |>
  ggplot(aes(y = genre, x = n, fill = service)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    facet_wrap(~ service, scales = "free") +
  labs(
    title = "Top Genres 10 of TV Shows by Streaming Services"
  ) + 
  theme_bw()
```

The genres are all pretty similar for movies between our streaming services with Drama, Comedy, and Action taking the top 3 for most titles with that genre, and Thriller, Romance, and Adventure for 4th, 5th and 6th, depending on the streaming service. For TV shows we see more of a variety for top 10 genres, we see an increase in Animation, Reality-TV, Crime, Documentary, and Family, however the genres are still very similar by streaming service. 

# Locations

Using another str_split() we can split up the lists of locations that the title is available to, titles like *The Queen's Gambit* is available to countries such as AD, AE, AG, AL, AO, AR, AT, AU, AZ, BA, BB, BE, BG and much more. After splitting our genres we find that we have 138 distinct countries that our streaming services are available in. We'll now look what countries offer the most titles. 

```{r, out.height= "80%"}
distinctcountry <- big_data |>
  mutate(country = str_split(availableCountries, ", ")) |>
  unnest(country) |>
  distinct(country)

big_data |>
  mutate(country = str_split(availableCountries, ", ")) |>
  unnest(country) |>
  count(country) |>
  slice_max(n, n = 10) |>
  ggplot(aes(x = fct_reorder(country, n), y = n)) +
    geom_bar(stat = "identity", show.legend = FALSE, fill = "orange") +
  labs(
    title = "Top 10 Locations",
    y = "Country",
    x = "Count"
  ) + 
  theme_bw()
```

The country with the most titles available is the US, then followed by Great Britain and Guernsey. 

```{r, out.height= "80%"}
big_data |>
  mutate(country = str_split(availableCountries, ", ")) |>
  unnest(country) |>
  group_by(service) |>
  count(country) |>
  slice_max(n, n = 10) |>
  ggplot(aes(x = fct_reorder(country, n), y = n, fill = service)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    facet_wrap(~ service, scales = "free") +
  labs(
    title = "Top 10 Locations for Titles Available by Service",
    y = "Country",
    x = "Count"
  ) + 
  theme_bw()
```

This is very interesting because I did not know that Hulu is only available in the US and Japan before this project. We can see that Japan has more titles available compared to the US. The US is the has the most titles available for Apple TV and Amazon Prime. Slovakia for Netflix, and Nicaragua for HBO. 

# Length & Content of Titles

## Length of Title

For the length of the titles, I am interested if titles the are longer or shorter over time, we will do this by looking at the word count of the titles and the actual character length of the title. By using str_length() we can calculate the how many characters are in the title and str_count() to count the number of words in our title. 

```{r}
# new tbl for numerical data of title
nums_big_data <- big_data |>
  
  # calculates length
  mutate(title_length = str_length(title), 
         title_lower = str_to_lower(title),
         
         # looks for words
         num_count = str_count(title_lower, "\\w+"), 
         
         # matches all numbers in title
         has_number = str_extract_all(title_lower, "\\d+") 
         )
```

```{r, warning = FALSE}
# function for scatter plot
lengthword_plot <- function(xaxis, yaxis, linemethod){
  nums_big_data |>
  group_by(releaseYear) |>
  drop_na() |>
  # mean of both to use
  summarize(avg_words = mean(num_count),
            avg_title = mean(title_length)) |>
  ggplot(aes(x = {{xaxis}}, y = {{yaxis}})) +
  geom_point() +
  geom_smooth(se = FALSE, show.legend = FALSE, formula = 'y ~ x', method = linemethod)
}

# plot for release year and average length of title
length_plot <- lengthword_plot(releaseYear, avg_title, loess) + 
  labs(
    title = "Average Length of Titles by Year",
    x = "Year",
    y = "Average Length of Title"
  ) +
  theme_bw()

# plot for release year and average number of words of title
word_plot <- lengthword_plot(releaseYear, avg_words, loess) + 
  labs(
    title = "Average Number of Words of Titles by Year",
    x = "Year",
    y = "Average Number of Words"
  ) +
  theme_bw()

# comparing avg words, and avg length
word_length <- lengthword_plot(avg_words, avg_title, lm) + 
  labs(
    title = "Average Length by Average Number of Words of Titles",
    x = "Average Number of Words",
    y = "Average Length"
  ) +
  theme_bw()

top_row <- plot_grid(length_plot, word_plot)

plot_grid(top_row, word_length, ncol = 1)
```

The average length of the title stays consistent over time from average length of 17ish characters in 1900 to 2024. We can see that our line of best fit is nearly identical since the length is depends on the words. The average title is around 3.4 words in 2024. This means that the average word is around 5 characters since the average length of our title is 17 in 2024 and the average number of words is 3.4. However, these plots don't tell us too much important information about our titles other than the fact that they are consistently around 3 to 4 words throughout the years. 

Something more exciting would be exploring the content our titles, these are the top ten most common words in our titles. 

```{r, echo = FALSE}
# getting every word from title
big_data_tidy <- big_data |>
  unnest_tokens(output = word, input = title)
```

```{r, echo = FALSE}
big_data_tidy |>
  count(word) |>
  drop_na(word) |>
  # rid of stop words
  anti_join(stop_words, by = join_by(word)) |>
  slice_max(n, n = 10) |>
  mutate(rank = 1:10) |>
  select(rank, word, n) |>
  # kable for nice table in html
  kable() |>
    kable_styling(full_width = FALSE) |>
    scroll_box(width = "100%", height = "200px")
```

We can see that "love" is our most common word, appearing 2165 times and then "christmas" appearing 1477 times as second most common. However, something interesting is that "2" is the third most common, this could mean that there are lots of sequels! If you watch a movie or show you enjoy it's very likely your streaming service also contains the sequel. Let's see what other numbers are common in our titles. These are the top 10 numbers that show up in our titles.

```{r, echo = FALSE}
nums_big_data |>
  unnest(has_number) |>
  drop_na(has_number) |>
  count(has_number) |>
  slice_max(n, n = 10) |>
  mutate(rank = 1:10) |>
  select(rank, has_number, n) |>
  kable() |>
    kable_styling(full_width = FALSE) |>
    scroll_box(width = "100%", height = "200px")
```

## Sequels

Let's see how many of these titles with a 2 in them are sequels and which sequels are available on our streaming services. Looking at the list of titles with 2's there were some common occurrences with how second movies are title. We see that there are titles that have a 2 and a colon, such as *Super Size Me 2: ...* or *Goosebumps 2: ...*, then there are the 'Vol. 2', 'VOL. 2' and 'Volume 2'. We also see some titles with 'Part 2' or 'Part 2:' however our OR expression will take care of the ones with a colon. Lastly, we have the titles ending with '2', like *Despicable Me 2* or *Shrek 2*. This makes sure that we don't pick up titles like *2 Fast 2 Furious*, *2:22* or *2 Women*. Using our regular expression we are able locate from our 1513 titles with 2's in them 1240 titles that are sequels. Below is a table of all the sequels available from the 5 streaming services, see if you can find a sequel you recognize.

```{r, echo = FALSE}
has_2 <- nums_big_data |>
  unnest(has_number) |>
  drop_na(has_number) |>
  filter(has_number == 2)
```

```{r, include = FALSE}
# 336 titles like "Super Size Me 2: ..." or "Goosebumps 2: "
# we don't want it to be "2: " since that will keep titles like 
# "U2: A Rock Crusade" or "B2: Stealth at War" which we don't want
str_subset(has_2$title, " 2: ")

# 11 titles with VOL.2, Vol.2, vol.2, vol 2 etc. 
str_subset(has_2$title, "[vV][oO][lL].2")

# 7 titles with Volume 2, Volume2
str_subset(has_2$title, "[vV]olume.2")

# 30 titles with movie 2 or Movie 2, lots of overlap with Movie 2:
str_subset(has_2$title, "[mM]ovie.2")

# 63 titles with Part 2 or part 2
str_subset(has_2$title, "[pP]art.2")

# 886 titles ends with a 2
str_subset(has_2$title, " 2$")

# 336+11+7+30+63+886 = 1333

#all together, 1240 titles
str_subset(has_2$title, " 2: |[vV][oO][lL].2|[vV]olume.2|[mM]ovie.2|[pP]art.2| 2$")
```

```{r}
sequels <- has_2 |>
  mutate(sequel = str_detect(title, " 2: |[vV][oO][lL].2|[vV]olume.2|[mM]ovie.2|[pP]art.2| 2$"))  |>
  filter(sequel == TRUE) 
```

```{r, echo = FALSE}
sequels |>
  select(title, service) |>
  kable() |>
    kable_styling(full_width = FALSE) |>
    scroll_box(width = "100%", height = "200px")
```

Now, we have a new tibble of just the titles that are sequels from our dataset. We already know that since Amazon Prime has the most titles, it's very likely that it will also have the most sequels, So, let's also look at which streaming service has the highest proportion of sequels.

```{r, out.height= "75%"}
sequels |>
  group_by(service) |>
  count() |>
  ggplot(aes(x = service, y = n, fill = service)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    geom_text(aes(label = n)) +
    theme() + 
    labs(
     title = "Sequels by Streaming Service",
     x = "Service",
     y = "Count"
   ) +
  theme_bw()
```

```{r, out.height= "75%"}
big_data |>
  mutate(sequel = str_detect(title, " 2: |[vV][oO][lL].2|[vV]olume.2|[mM]ovie.2|[pP]art.2| 2$")) |>
  group_by(service) |>
  
  # calculate proportion
  summarize(proportion_sequels = mean(sequel, na.rm = TRUE)) |>
  ggplot(aes(x = service, 
             y = proportion_sequels, 
             fill = service)) +
  geom_bar(stat = "identity", 
           show.legend = FALSE) +
  geom_text(aes(label = round(proportion_sequels, 5))) +
  theme() + 
  labs(
     title = "Proportion of Sequels by Streaming Service",
     x = "Service",
     y = "Proportion"
     ) +
  theme_bw()
```

We can see that even though Amazon Prime has the most titles that are sequels, it doesn't mean it has the highest proportion of sequels. Netflix has the highest proportion with 0.015% of the titles on Netflix are sequels. 

## Sentiment: Joy and Sadness

Now we'll look at the list of the top 75 words associated with "joy" and with "sadness" (according to the "nrc" lexicon) across all the titles.

```{r}
nrc_sentiments <- get_sentiments(lexicon = "nrc")

get_sentiment <- function(emotion1, emotion2){
  nrc_sentiments |>
    
  # filter for two emotions we want
  filter(sentiment == {{emotion1}} | sentiment == {{emotion2}}) |>
  
  # join back in with all words from title 
  inner_join(big_data_tidy, relationship = "many-to-many", by = join_by(word)) |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) 
}

#use function
joy_sad <- get_sentiment("joy", "sadness")

joy_sad_cloud <- joy_sad |>
  slice_max(n, n = 75) |>
  
  # create color variable 
  mutate(color = ifelse(sentiment == "joy", "lightpink", "lightblue")) |>
  ungroup()
```

```{r}
set.seed(2222)
wordcloud(
    words = joy_sad_cloud$word,
    freq = joy_sad_cloud$n,
    random.order = FALSE,
    ordered.colors = TRUE,
    color = joy_sad_cloud$color,
    scale = c(3.5, 1),
    rot.per = .50)
```

Are there more words associated with joy or with sadness in our titles?

```{r}
joy_sad |>
  count(sentiment) |>
  ggplot(aes(x = sentiment, y = n)) +
  geom_bar(stat = "identity", fill = c("lightpink", "lightblue")) +
  geom_text(aes(label = n)) +
  labs(
    title = "Count of Joy and Sadness",
    x = "Sentiment",
    y = "Count"
  ) +
  theme_bw()
```

There are more about 200 more words associated with sadness compared to joy in our titles. 

## Sentiment: Fear and Trust

What about words associated with fear and trust? 

```{r}
#use function
fear_trust <- get_sentiment("fear", "trust")

fear_trust_cloud <- fear_trust |>
  slice_max(n, n = 75) |>
  # create color variable 
  mutate(color = ifelse(sentiment == "fear", "#CE2029", "#228B22")) |>
  ungroup()

set.seed(2222)
wordcloud(
    words = fear_trust_cloud$word,
    freq = fear_trust_cloud$n,
    random.order = FALSE,
    ordered.colors = TRUE,
    color = fear_trust_cloud$color,
    scale = c(2.5, .5),
    rot.per = .50)
```

Are there more words associated with joy or with sadness in our titles?

```{r}
fear_trust |>
  count(sentiment) |>
  ggplot(aes(x = sentiment, y = n)) +
  geom_bar(stat = "identity", fill = c("#CE2029", "#228B22")) +
  geom_text(aes(label = n)) +
  labs(
    title = "Count of Fear and Trust",
    x = "Sentiment",
    y = "Count"
  ) +
  theme_bw()
```

There are more about 120 more words associated with fear compared to trust in our titles. 

## Sentiment: Positive/Negative trajectory of Top 4 Genres by Year

The bing lexicon dataset has 6,786 words that are identidied as negative or postive, for the genres Action, Comedy, Drama, and Thriller we're going to count the number of negative or positive words and calculate the what we call the sentiment value which is the number of positive words minus the negative words. Our bar will extend up from 0 if there are more positive than negative words in a year and it will extend down from 0 if there are more negative than positive words.

```{r}
bigger_data_tidy <- big_data_tidy |>
  mutate(genres = str_split(genres, ", | & ")) |> 
  unnest(genres) |> 
  drop_na(genres)

# load in bing sentiments
bing_sentiments <- get_sentiments(lexicon = "bing")

bing_sentiments |>
  inner_join(bigger_data_tidy, relationship = "many-to-many", by = join_by(word)) |>
  
  # count by genre, year, sentiment
  count(genres, releaseYear, sentiment, sort = TRUE) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  
  #create sentiment value
  mutate(sentiment = positive - negative) |>
  filter(genres %in% c("Drama", "Comedy", "Action", "Thriller")) |>
  drop_na() |>
  ggplot(aes(x = releaseYear, y = sentiment, fill = genres)) +
  geom_col(show.legend = FALSE) +
    facet_wrap(~genres, ncol = 2, scales = "free_x") +
  labs(
    title = "Sentiment Trajectory of Top 4 Genres by Year",
    x = "Release Year",
    y = "Sentiment Value"
  ) +
  theme_bw()
```

We can see that for Action and Thriller the sentiment values are very negative, and for Drama there are 2 years in which the titles have more postive words than negative. For Comedy, it is almost split between negative and positive, which is nice to see since most would consider comedic movie or tv show be light-hearted.

# LDA 2-topic model 

The last thing we will do is create LDA model. LDA stands for Latent Dirichlet allocation, it one of the most common algorithms for topic modeling. We need two things to make it work.

1) Every document is a mixture of topics, this is where each row represents one document (such as a book or article), in our case it would be a genre. Each document can contain words from several topics. For example, in a two-topic model we could observe “Document 1 is 75% topic A and 25% topic B, while Document 2 is 34% topic A and 66% topic B.”

2) Every topic is a mixture of words, imagine a two-topic model with sports, Each topic would have its own set of words that define it. Here’s an example of what these could look like: 

Topic 1 Words (Competitive & Physical Sports): "win", "competition", "athlete", "team", "championship" "playoff"

Topic 2 Words (Lifestyle & Recreational Sports): "fitness" "health" "leisure" "adventure" "outdoor" "relaxation"

However, it is important to note that words can be shared between topics; a word like “game”, strength" or "exercise" might appear in both topics equally.

For our dataset, we are interested in our genres and how the distribution of words within the titles of each genre reflects the themes and characteristics of different types of movies and TV shows. After filtering for stop words we will create a dtm for our words and genres, fit our lda model and then interpret our findings. 

```{r}
# creating dtm matrix
bigger_data_dtm <- bigger_data_tidy |>
  filter(!word %in% stop_words$word,
         !is.na(word)) |>
  group_by(genres) |>
  count(word) |>
  cast_dtm(genres, word, n)

# fitting the model
bigger_data_lda <- LDA(bigger_data_dtm, k = 2, control = list(seed = 2222))

bigger_data_topics <- tidy(bigger_data_lda, matrix = "beta")
```

A table of our highest beta value, which is the probability of a word being generated by a specific topic.

```{r, echo = FALSE}
#table for 50 words
bigger_data_topics |>
  arrange(-beta) |>
  slice_max(beta, n = 50) |>
  kable() |>
    kable_styling(full_width = FALSE) |>
    scroll_box(width = "100%", height = "200px")

# # Find the most common words within each topic
# bigger_data_top_terms <- bigger_data_topics |>
#   group_by(topic) |>
#   slice_max(beta, n = 10) |>
#   ungroup() |>
#   arrange(topic, -beta)

# Find words with greatest difference between two topics, using log ratio
bigger_beta_wide <- bigger_data_topics |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |> 
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1))
```

```{r}
# graph of words
bigger_beta_wide |>
  arrange(desc(abs(log_ratio))) |>
  slice_max(abs(log_ratio), n = 20) |>
  mutate(term = reorder(term, log_ratio)) |>
  ggplot(aes(log_ratio, term, fill = log_ratio > 0)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Log ratio of Beta values",
         y = "Words in Genres") +
  theme_bw()
```


These are the words with the greatest difference between the two topics, using the log ratio, looking at these words we can see sort of where these topics are leading us. 

```{r, echo = FALSE}
data_documents <- tidy(bigger_data_lda, matrix = "gamma")

# Find documents for each topic
data_documents |>
  group_by(topic) |>
  slice_max(gamma, n = 10) |>
  ungroup() |>
  arrange(topic, -gamma) |>
  kable()
```

Topic 1 (red):
Documents heavily associated with Topic 1 include genres like Adventure, Romance, Animation, Family, Fantasy, Musical, Game-Show, Comedy, Adult, and Kids. This topic seems to represent genres that are typically focused on entertainment, family-friendly content, and lighter themes. There is a range of genres that cater to various age groups and include both animated and live-action content. Topics in this category could focus on fun, adventure, humor, and fictional worlds.

Topic 2 (blue):
Documents heavily associated with Topic 2 include genres like Documentary, Crime, Thriller, Horror, Mystery, Biography, History, War, Film-Noir, and News. This topic seems to capture genres that are more serious, dramatic, and often grounded in real-world issues, with a focus on storytelling, historical events, real-life experiences, and suspense. These genres often explore darker, more complex, or informative themes.

# Conclusion

In conclusion, this analysis provides an exploration of the genres and characteristics of movie and TV show titles across major streaming services such as Amazon Prime, Apple TV, HBO MAX, Hulu, and Netflix. By examining over 120,000 titles, we identified patterns in genre distribution, locations available, title length trends over time, and sentiment variations across genres.

The results reveal that Drama, Comedy, and Action dominate the movie genres across all platforms, with TV shows offering a broader variety including Animation, Reality-TV, and Documentary.

Our topic modeling analysis further distinguishes two overarching themes: family-friendly entertainment genres like Comedy and Adventure, and serious, real-world genres like Crime and Documentary. This highlights the diversity in content and the ability of streaming platforms to cater to both lighthearted and thought-provoking preferences.

We gain a better understanding of the types of content available on streaming services by extracting data from the titles and plotting the results.
